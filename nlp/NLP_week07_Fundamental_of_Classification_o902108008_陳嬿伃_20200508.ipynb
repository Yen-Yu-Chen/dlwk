{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然語言處理與文字探勘技術：Fundamental of Classification\n",
    "## 課程練習(Homework week 07)\n",
    "\n",
    "### 姓名（Name）：陳嬿伃\n",
    "### 學號（Student ID）：o902108008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：教學及教材的實作\n",
    "## Task I: Hands On Practice\n",
    "\n",
    "### 請就實體課程，線上教學，教材等提到的實作，依序於此實際動手寫程式並執行，就結果討論。\n",
    "### refer \"Fundamental of Classification.pdf\" and complete \"LET'S CODE\" practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting label and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset, page 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "def get_dataset(file_path):\n",
    "    workbook = xlrd.open_workbook(file_path)\n",
    "    booksheet = workbook.sheet_by_name('training')\n",
    "    text = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range(booksheet.nrows):\n",
    "        gender = booksheet.cell(i, 1).value\n",
    "        if gender == 'M':\n",
    "            label.append(\"0\")\n",
    "        else:\n",
    "            label.append(\"1\")\n",
    "        text.append(booksheet.cell(i, 0).value)\n",
    "    return label, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Implementation of Text Classifier, page 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold #做幾等分的交叉驗證用\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "labels, texts = get_dataset(\"./blog-gender-dataset.xlsx\") #這裡暫不考慮stopword\n",
    "tfidf_BOW = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                           max_df =0.9, min_df = 0.1)\n",
    "tfidf_BOW.fit_transform(texts) #把文字清單轉成向量\n",
    "\n",
    "predicted = []\n",
    "expected = []\n",
    "for train_index, test_index in KFold(n_splits = 10, shuffle = True).split(texts):\n",
    "    x_train = np.array(texts)[train_index]\n",
    "    y_train = np.array(labels)[train_index]\n",
    "\n",
    "    x_test = np.array(texts)[test_index]\n",
    "    y_test = np.array(labels)[test_index]\n",
    "\n",
    "    vectors_training = tfidf_BOW.fit_transform(x_train)\n",
    "    vectors_test = tfidf_BOW.transform(x_test)\n",
    "    \n",
    "    model = MultinomialNB(alpha = .01)\n",
    "    model.fit(vectors_training, y_train) #fit訓練資料\n",
    "    \n",
    "    expected.extend(y_test)\n",
    "    predicted.extend(model.predict(vectors_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation, page 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.6325767123262274\n",
      "Micro-average: 0.6395420792079208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.52      0.58      1547\n",
      "           1       0.63      0.75      0.68      1685\n",
      "\n",
      "    accuracy                           0.64      3232\n",
      "   macro avg       0.64      0.63      0.63      3232\n",
      "weighted avg       0.64      0.64      0.63      3232\n",
      "\n",
      "[[ 811  736]\n",
      " [ 429 1256]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Macro-average: {0}\".format(metrics.f1_score(expected, predicted, average='macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(expected, predicted, average='micro')))\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 20 Newsgroups Dataset (2/3), page 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "###The 20 Newsgroups Dataset (資料在sklearn裡面就有)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n",
      "(11314,)\n",
      "(11314,)\n",
      "[ 7  4  4  1 14 16 13  3  2  4]\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(fetch_20newsgroups))\n",
    "print(newsgroups_train.filenames.shape)\n",
    "print(newsgroups_train.target.shape)\n",
    "print(newsgroups_train.target[:10])\n",
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 20 Newsgroups Dataset (3/3), page 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'sci.space']\n",
      "(1073,)\n",
      "(1073,)\n",
      "[0 1 1 1 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "#為了方便先選則兩個做為範例\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train', categories=cats)\n",
    "print(list(newsgroups_train.target_names))\n",
    "print(newsgroups_train.filenames.shape)\n",
    "print(newsgroups_train.target.shape)\n",
    "print(newsgroups_train.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic classification using Multinomial NB with TF-IDF representation (1/3), page 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.8821359240272957\n",
      "Micro-average: 0.893569844789357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       319\n",
      "           1       0.95      0.95      0.95       389\n",
      "           2       0.91      0.96      0.93       394\n",
      "           3       0.85      0.76      0.80       251\n",
      "\n",
      "    accuracy                           0.89      1353\n",
      "   macro avg       0.89      0.88      0.88      1353\n",
      "weighted avg       0.89      0.89      0.89      1353\n",
      "\n",
      "[[274   2   9  34]\n",
      " [  5 368  16   0]\n",
      " [  2  15 377   0]\n",
      " [ 47   3  11 190]]\n"
     ]
    }
   ],
   "source": [
    "# Topic classification using Multinomial NB with TF-IDF representation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "trainingData = fetch_20newsgroups(subset = 'train', categories = categories)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors_training = vectorizer.fit_transform(trainingData.data)\n",
    "\n",
    "#training process\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics #轉成向量後拿測試資料\n",
    "testData = fetch_20newsgroups(subset = 'test', categories = categories)\n",
    "vectors_test =  vectorizer.transform(testData.data)\n",
    "model = MultinomialNB(alpha = .01)\n",
    "model.fit(vectors_training, trainingData.target)\n",
    "\n",
    "#text process\n",
    "predicted = model.predict(vectors_test)\n",
    "print(\"Macro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='micro')))\n",
    "print(metrics.classification_report(testData.target, predicted))\n",
    "print(metrics.confusion_matrix(testData.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fetch_20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic classification using Multinomial NB (2/3 ~ 3/3), page 79, 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.8290659644474043\n",
      "Micro-average: 0.8352363250132767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       319\n",
      "           1       0.69      0.75      0.72       389\n",
      "           2       0.74      0.63      0.68       394\n",
      "           3       0.65      0.75      0.69       392\n",
      "           4       0.83      0.84      0.83       385\n",
      "           5       0.84      0.78      0.81       395\n",
      "           6       0.82      0.78      0.80       390\n",
      "           7       0.89      0.90      0.90       396\n",
      "           8       0.93      0.96      0.95       398\n",
      "           9       0.95      0.94      0.95       397\n",
      "          10       0.95      0.97      0.96       399\n",
      "          11       0.89      0.93      0.91       396\n",
      "          12       0.79      0.77      0.78       393\n",
      "          13       0.89      0.84      0.86       396\n",
      "          14       0.87      0.91      0.89       394\n",
      "          15       0.82      0.95      0.88       398\n",
      "          16       0.76      0.91      0.83       364\n",
      "          17       0.97      0.94      0.96       376\n",
      "          18       0.80      0.64      0.71       310\n",
      "          19       0.76      0.59      0.67       251\n",
      "\n",
      "    accuracy                           0.84      7532\n",
      "   macro avg       0.83      0.83      0.83      7532\n",
      "weighted avg       0.84      0.84      0.83      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic classification using Multinomial NB\n",
    "###資料全做，不做篩選的情況\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "trainingData = fetch_20newsgroups(subset = 'train')\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors_training = vectorizer.fit_transform(trainingData.data)\n",
    "\n",
    "#training process\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "testData = fetch_20newsgroups(subset = 'test')\n",
    "vectors_test =  vectorizer.transform(testData.data)\n",
    "model = MultinomialNB(alpha = .01)\n",
    "model.fit(vectors_training, trainingData.target)\n",
    "\n",
    "#text procecc\n",
    "predicted = model.predict(vectors_test)\n",
    "print(\"Macro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='micro')))\n",
    "print(metrics.classification_report(testData.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[249   0   0   4   0   1   0   0   1   1   0   1   0   5   5  28   3   3\n",
      "    1  17]\n",
      " [  0 290  15  14  10  23   6   0   0   3   0   4  12   0   7   2   0   2\n",
      "    0   1]\n",
      " [  1  32 248  52   4  20   5   0   2   1   1   6   3   3   5   4   0   0\n",
      "    4   3]\n",
      " [  0  11  26 293  22   1  11   1   0   1   0   1  21   0   4   0   0   0\n",
      "    0   0]\n",
      " [  0   7  10  14 322   1   8   4   1   2   1   2   9   2   1   0   1   0\n",
      "    0   0]\n",
      " [  0  40  14  11   6 307   3   1   2   0   0   3   2   1   4   0   1   0\n",
      "    0   0]\n",
      " [  0   4   6  26   8   0 306  11   9   1   5   0   9   4   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   5   1   0  10 358   6   1   0   0   6   3   1   0   2   0\n",
      "    1   0]\n",
      " [  0   1   0   1   1   0   2   7 383   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   3   4   0 373  11   1   0   0   2   0   0   2\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   0   0   4 387   2   0   1   0   2   1   0\n",
      "    0   0]\n",
      " [  1   3   1   2   2   1   3   3   0   0   0 370   1   3   2   0   4   0\n",
      "    0   0]\n",
      " [  1   9   9  23   6   2   7   3   2   0   0  13 302   9   5   0   0   1\n",
      "    1   0]\n",
      " [  2  10   1   3   1   3   3   4   1   2   0   4   8 332   2   7   1   2\n",
      "    8   2]\n",
      " [  1   8   0   3   1   3   1   1   0   0   0   2   3   5 359   2   1   0\n",
      "    4   0]\n",
      " [  3   1   1   1   0   0   0   0   1   1   1   0   0   2   1 378   0   0\n",
      "    2   6]\n",
      " [  0   0   0   1   0   0   1   0   2   1   0   5   1   1   1   0 331   0\n",
      "   14   6]\n",
      " [  5   1   0   0   0   1   0   0   0   1   1   0   0   0   0   2   2 355\n",
      "    7   1]\n",
      " [  4   1   0   0   2   0   1   4   0   0   1   3   0   2   9   2  72   0\n",
      "  199  10]\n",
      " [ 35   1   2   0   0   0   0   0   0   0   0   1   0   2   5  33  15   1\n",
      "    7 149]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(testData.target, predicted)) #全部的資料矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic classification using NB with BOW representation (1/2 ~ 2/2), page 81, 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.7852092132952866\n",
      "Micro-average: 0.8039033457249071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.82       319\n",
      "           1       0.57      0.78      0.66       389\n",
      "           2       0.75      0.04      0.07       394\n",
      "           3       0.55      0.78      0.64       392\n",
      "           4       0.74      0.83      0.78       385\n",
      "           5       0.80      0.73      0.76       395\n",
      "           6       0.79      0.85      0.82       390\n",
      "           7       0.86      0.90      0.88       396\n",
      "           8       0.91      0.96      0.94       398\n",
      "           9       0.95      0.93      0.94       397\n",
      "          10       0.96      0.96      0.96       399\n",
      "          11       0.88      0.93      0.91       396\n",
      "          12       0.77      0.76      0.76       393\n",
      "          13       0.88      0.83      0.86       396\n",
      "          14       0.87      0.89      0.88       394\n",
      "          15       0.89      0.92      0.91       398\n",
      "          16       0.80      0.89      0.84       364\n",
      "          17       0.97      0.89      0.93       376\n",
      "          18       0.75      0.64      0.69       310\n",
      "          19       0.67      0.65      0.66       251\n",
      "\n",
      "    accuracy                           0.80      7532\n",
      "   macro avg       0.81      0.80      0.79      7532\n",
      "weighted avg       0.81      0.80      0.79      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topic classification using NB with BOW representation\n",
    "###資料全做，不做篩選的情況\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer #改這裡跟上面來比較\n",
    "trainingData = fetch_20newsgroups(subset = 'train')\n",
    "vectorizer = CountVectorizer() #改這裡\n",
    "vectors_training = vectorizer.fit_transform(trainingData.data)\n",
    "\n",
    "#training process\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "testData = fetch_20newsgroups(subset = 'test')\n",
    "vectors_test =  vectorizer.transform(testData.data)\n",
    "model = MultinomialNB(alpha = .01)\n",
    "model.fit(vectors_training, trainingData.target)\n",
    "\n",
    "#text procecc\n",
    "predicted = model.predict(vectors_test)\n",
    "print(\"Macro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='micro')))\n",
    "print(metrics.classification_report(testData.target, predicted))\n",
    "\n",
    "\n",
    "#效果比TF-IDF差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[265   1   0   4   0   0   0   0   2   2   0   2   0   2   2  14   2   3\n",
      "    0  20]\n",
      " [  1 304   1  16  13  17   8   0   0   1   0   8  13   0   5   2   0   0\n",
      "    0   0]\n",
      " [  1  87  15 146  39  44   9   3   4   1   1   6   9   8   6   2   2   0\n",
      "    7   4]\n",
      " [  0   8   1 307  30   2  11   1   0   1   0   2  26   0   3   0   0   0\n",
      "    0   0]\n",
      " [  0  11   1  16 320   1  15   2   2   2   0   1  11   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0  66   2  14   7 288   5   2   3   0   0   2   2   1   3   0   0   0\n",
      "    0   0]\n",
      " [  0   6   0  20   9   0 330  10   3   1   3   0   4   1   2   0   1   0\n",
      "    0   0]\n",
      " [  0   1   0   2   1   0  12 355  11   1   0   0   7   2   1   0   2   0\n",
      "    1   0]\n",
      " [  0   0   0   1   1   0   3   9 382   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   1   0   0   1   0   4   3   0 369   9   0   0   3   4   0   0   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   1   1   1   0   5 385   1   0   1   0   1   0   0\n",
      "    2   1]\n",
      " [  1   4   0   3   2   2   3   3   0   1   0 367   1   2   2   0   5   0\n",
      "    0   0]\n",
      " [  1  16   0  26   8   1   9   6   4   0   0  13 297   5   4   0   0   1\n",
      "    1   1]\n",
      " [  2  10   0   2   1   0   4   7   3   3   0   2   9 329   4   3   0   3\n",
      "   10   4]\n",
      " [  2  13   0   3   0   3   2   1   0   0   0   1   6   3 351   3   0   0\n",
      "    6   0]\n",
      " [  3   2   0   1   0   1   0   0   0   1   2   0   0   2   2 368   0   0\n",
      "    1  15]\n",
      " [  0   0   0   1   0   0   1   1   4   0   0   3   1   3   0   0 325   1\n",
      "   13  11]\n",
      " [ 12   2   0   0   0   0   0   0   0   1   1   2   0   1   0   2   3 336\n",
      "   15   1]\n",
      " [  4   1   0   0   1   0   1   5   1   0   1   4   0   5   8   2  56   0\n",
      "  199  22]\n",
      " [ 37   3   0   0   0   0   0   2   0   0   0   1   0   2   5  15  12   1\n",
      "   10 163]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(testData.target, predicted)) #全部的資料矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified procedures using Pipeline, page 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.8290659644474043\n",
      "Micro-average: 0.8352363250132767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80       319\n",
      "           1       0.69      0.75      0.72       389\n",
      "           2       0.74      0.63      0.68       394\n",
      "           3       0.65      0.75      0.69       392\n",
      "           4       0.83      0.84      0.83       385\n",
      "           5       0.84      0.78      0.81       395\n",
      "           6       0.82      0.78      0.80       390\n",
      "           7       0.89      0.90      0.90       396\n",
      "           8       0.93      0.96      0.95       398\n",
      "           9       0.95      0.94      0.95       397\n",
      "          10       0.95      0.97      0.96       399\n",
      "          11       0.89      0.93      0.91       396\n",
      "          12       0.79      0.77      0.78       393\n",
      "          13       0.89      0.84      0.86       396\n",
      "          14       0.87      0.91      0.89       394\n",
      "          15       0.82      0.95      0.88       398\n",
      "          16       0.76      0.91      0.83       364\n",
      "          17       0.97      0.94      0.96       376\n",
      "          18       0.80      0.64      0.71       310\n",
      "          19       0.76      0.59      0.67       251\n",
      "\n",
      "    accuracy                           0.84      7532\n",
      "   macro avg       0.83      0.83      0.83      7532\n",
      "weighted avg       0.84      0.84      0.83      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simplified procedures using Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline #修改這裡\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "model = Pipeline([('tfidf', TfidfVectorizer()),('clf', MultinomialNB(alpha=.01))]) #修改這裡\n",
    "trainingData = fetch_20newsgroups(subset = 'train')\n",
    "testData = fetch_20newsgroups(subset = 'test')\n",
    "model.fit(trainingData.data, trainingData.target)\n",
    "predicted = model.predict(testData.data)\n",
    "print(\"Macro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='micro')))\n",
    "print(metrics.classification_report(testData.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[249   0   0   4   0   1   0   0   1   1   0   1   0   5   5  28   3   3\n",
      "    1  17]\n",
      " [  0 290  15  14  10  23   6   0   0   3   0   4  12   0   7   2   0   2\n",
      "    0   1]\n",
      " [  1  32 248  52   4  20   5   0   2   1   1   6   3   3   5   4   0   0\n",
      "    4   3]\n",
      " [  0  11  26 293  22   1  11   1   0   1   0   1  21   0   4   0   0   0\n",
      "    0   0]\n",
      " [  0   7  10  14 322   1   8   4   1   2   1   2   9   2   1   0   1   0\n",
      "    0   0]\n",
      " [  0  40  14  11   6 307   3   1   2   0   0   3   2   1   4   0   1   0\n",
      "    0   0]\n",
      " [  0   4   6  26   8   0 306  11   9   1   5   0   9   4   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   5   1   0  10 358   6   1   0   0   6   3   1   0   2   0\n",
      "    1   0]\n",
      " [  0   1   0   1   1   0   2   7 383   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   3   4   0 373  11   1   0   0   2   0   0   2\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   0   0   4 387   2   0   1   0   2   1   0\n",
      "    0   0]\n",
      " [  1   3   1   2   2   1   3   3   0   0   0 370   1   3   2   0   4   0\n",
      "    0   0]\n",
      " [  1   9   9  23   6   2   7   3   2   0   0  13 302   9   5   0   0   1\n",
      "    1   0]\n",
      " [  2  10   1   3   1   3   3   4   1   2   0   4   8 332   2   7   1   2\n",
      "    8   2]\n",
      " [  1   8   0   3   1   3   1   1   0   0   0   2   3   5 359   2   1   0\n",
      "    4   0]\n",
      " [  3   1   1   1   0   0   0   0   1   1   1   0   0   2   1 378   0   0\n",
      "    2   6]\n",
      " [  0   0   0   1   0   0   1   0   2   1   0   5   1   1   1   0 331   0\n",
      "   14   6]\n",
      " [  5   1   0   0   0   1   0   0   0   1   1   0   0   0   0   2   2 355\n",
      "    7   1]\n",
      " [  4   1   0   0   2   0   1   4   0   0   1   3   0   2   9   2  72   0\n",
      "  199  10]\n",
      " [ 35   1   2   0   0   0   0   0   0   0   0   1   0   2   5  33  15   1\n",
      "    7 149]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(testData.target, predicted)) #全部的資料矩陣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 20 Newsgroups Dataset: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20Newsgroup Dataset using SVM, page 108, 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average: 0.8849384370085389\n",
      "Micro-average: 0.8965262379896525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       319\n",
      "           1       0.90      0.98      0.94       389\n",
      "           2       0.96      0.94      0.95       394\n",
      "           3       0.81      0.79      0.80       251\n",
      "\n",
      "    accuracy                           0.90      1353\n",
      "   macro avg       0.89      0.88      0.88      1353\n",
      "weighted avg       0.90      0.90      0.90      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "trainingData = fetch_20newsgroups(subset = 'train', categories = categories)\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectors_training = vectorizer.fit_transform(trainingData.data)\n",
    "\n",
    "#training process\n",
    "from sklearn import svm ###改這行\n",
    "from sklearn import metrics\n",
    "testData = fetch_20newsgroups(subset = 'test', categories = categories)\n",
    "vectors_test =  vectorizer.transform(testData.data)\n",
    "model = svm.SVC(kernel = 'linear') ###改這行\n",
    "model.fit(vectors_training, trainingData.target)\n",
    "\n",
    "#text procecc\n",
    "predicted = model.predict(vectors_test)\n",
    "print(\"Macro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='macro')))\n",
    "print(\"Micro-average: {0}\".format(metrics.f1_score(testData.target, predicted, average='micro')))\n",
    "print(metrics.classification_report(testData.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[262  10   5  42]\n",
      " [  0 380   5   4]\n",
      " [  1  20 372   1]\n",
      " [ 35  11   6 199]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(testData.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小結論：\n",
    "1. KFold是為了將資料分成幾等分，做為交叉驗證使用\n",
    "2. sklearn支援很多種分類方式，只要一行就可以宣告模型，此外，亦可以修改幾行就換成另一種分類方式，很方便!\n",
    "3. 要做主題分類(Topic classification)的話，使用Multinomial NB並用TF-IDF呈現，其結果比用傳統的Bag-of-words呈現來的好\n",
    "4. Pipeline有點像是工廠，一步一步往下做\n",
    "5. 使用SVM分類結果較Naïve Bayes分類來的好\n",
    "6. SVM分類要找margin最大的，代表可以把資料分得很好\n",
    "7. SVM投射到更高維度的空間，找到比較好的解，使用Kernel Functions\n",
    "8. 在做文字分類之前的處理很重要，要篩選好資料再帶入模型，預測成果至少都有8.9成的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
